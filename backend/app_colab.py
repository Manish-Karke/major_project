# -*- coding: utf-8 -*-
"""API_CALL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_xPvxO0xZs8-TqsoNYdTCDWLSZasC1PB
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install huggingface_hub

from huggingface_hub import InferenceClient # Changed InterferenceClient to InferenceClient
import json
import os

os.environ["HF_TOKEN"] = "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" # Replace with your actual token

repo_id = "mistralai/Mistral-7B-Instruct-v0.3"

llm_client = InferenceClient( # Changed InterferenceClient to InferenceClient
    model = repo_id,
    token=os.environ["HF_TOKEN"],
    timeout = 120,
)

def call_llm(inference_client: InferenceClient, prompt: str): # Changed InterferenceClient to InferenceClient
    response = inference_client.post(
        json = {
            "inputs": prompt,
            "parameters": {"max_new_tokens": 250},
            "task": "text-generation",
        },
    )
    return json.loads(response.decode())[0]["generated_text"]

response = call_llm(llm_client, "what is manish") # Fixed typo: resposne to response and call
print(response)

